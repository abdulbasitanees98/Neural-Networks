{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"codec.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"7T-J9jjZxTJS","colab_type":"code","colab":{}},"source":["import torch\n","import torchvision\n","from torch import nn\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","class EncoderCNN(nn.Module):\n","    def __init__(self, embed_size=1024):\n","        super(EncoderCNN, self).__init__()\n","\n","        # get the pretrained densenet model\n","        self.densenet = torchvision.models.densenet121(pretrained=True)\n","\n","        # replace the classifier with a fully connected embedding layer\n","        self.densenet.classifier = nn.Linear(in_features=1024, out_features=1024)\n","\n","        # add another fully connected layer\n","        self.fulCon = nn.Linear(in_features=1024, out_features=embed_size)\n","\n","        # dropout layer\n","        self.dropout = nn.Dropout(p=0.5)\n","\n","        # activation layers\n","        self.prelu = nn.PReLU()\n","\n","    def forward(self, images):\n","\n","        # get the embeddings from the densenet\n","        densenet_outputs = self.dropout(self.prelu(self.densenet(images)))\n","\n","        # pass through the fully connected\n","        embeddings = self.fulCon(densenet_outputs)\n","\n","        return embeddings\n","\n","class DecoderRNN(nn.Module):\n","    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n","        super(DecoderRNN, self).__init__()\n","\n","        # define the properties\n","        self.embed_size = embed_size\n","        self.hidden_size = hidden_size\n","        self.vocab_size = vocab_size\n","\n","        # embedding layer\n","        self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_size)\n","\n","        # lstm cell\n","        self.lstm_cell = nn.LSTMCell(input_size=embed_size, hidden_size=hidden_size)\n","\n","        # output fully connected layer\n","        self.fc_out = nn.Linear(in_features=self.hidden_size, out_features=self.vocab_size)\n","\n","        # activations\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, features, captions):\n","        # batch size\n","        batch_size = features.size(0)\n","\n","        # init the hidden and cell states to zeros\n","        hidden_state = torch.zeros((batch_size, self.hidden_size)).to(device)\n","        cell_state = torch.zeros((batch_size, self.hidden_size)).to(device)\n","\n","        # define the output tensor placeholder\n","        outputs = torch.empty((batch_size, captions.size(1), self.vocab_size)).to(device)\n","\n","        # embed the captions\n","        captions_embed = self.embed(captions)\n","\n","        # pass the caption word by word\n","        for t in range(captions.size(1)):\n","\n","            # for the first time step the input is the feature vector\n","            if t == 0:\n","                hidden_state, cell_state = self.lstm_cell(features.to(device), (hidden_state, cell_state))\n","\n","            # for the 2nd+ time step, using teacher forcer\n","            else:\n","                hidden_state, cell_state = self.lstm_cell(captions_embed[:, t, :], (hidden_state, cell_state))\n","\n","            # output of the attention mechanism\n","            out = self.fc_out(hidden_state)\n","\n","            # build the output tensor\n","            outputs[:, t, :] = out\n","\n","        return outputs"],"execution_count":0,"outputs":[]}]}