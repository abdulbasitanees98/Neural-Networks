{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"n93eNFhASzlB","colab_type":"code","outputId":"2b512a9a-d931-458b-f5c6-0bbb7170d3ee","executionInfo":{"status":"ok","timestamp":1578567187187,"user_tz":-180,"elapsed":21966,"user":{"displayName":"Canberk Baykal","photoUrl":"","userId":"03340276410437477738"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jAMzZf5wTPJ6","colab_type":"code","outputId":"a218c9f3-88a2-4c6b-cb4f-d61a19c2807d","executionInfo":{"status":"ok","timestamp":1578567233827,"user_tz":-180,"elapsed":3159,"user":{"displayName":"Canberk Baykal","photoUrl":"","userId":"03340276410437477738"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["!ls /content/drive/My\\ Drive/Colab"],"execution_count":3,"outputs":[{"output_type":"stream","text":["cleanTrainCap  dataloader.ipynb\t\t\timages\n","cleanTrainID   data_prep.ipynb\t\t\tinvalidAllUrls\n","codec.ipynb    data_prep.py\t\t\tmain.ipynb\n","codec.py       eee443_project_dataset_train.h5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FeMZ4v0_wcsr","colab_type":"code","colab":{}},"source":["import torch\n","import os\n","from torch.utils.data import DataLoader\n","from torch import nn\n","from torchvision import transforms\n","from PIL import Image\n","import pandas as pd\n","#from codec import EncoderCNN, DecoderRNN\n","#from data_prep import ImageDataset"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OjH11XqYVQQ8","colab_type":"code","colab":{}},"source":["import torch\n","import torchvision\n","from torch import nn\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","class EncoderCNN(nn.Module):\n","    def __init__(self, embed_size=1024):\n","        super(EncoderCNN, self).__init__()\n","\n","        # get the pretrained densenet model\n","        self.densenet = torchvision.models.densenet121(pretrained=True)\n","\n","        # replace the classifier with a fully connected embedding layer\n","        self.densenet.classifier = nn.Linear(in_features=1024, out_features=1024)\n","\n","        # add another fully connected layer\n","        self.fulCon = nn.Linear(in_features=1024, out_features=embed_size)\n","\n","        # dropout layer\n","        self.dropout = nn.Dropout(p=0.5)\n","\n","        # activation layers\n","        self.prelu = nn.PReLU()\n","\n","    def forward(self, images):\n","\n","        # get the embeddings from the densenet\n","        densenet_outputs = self.dropout(self.prelu(self.densenet(images)))\n","\n","        # pass through the fully connected\n","        embeddings = self.fulCon(densenet_outputs)\n","\n","        return embeddings\n","    def fine_tune(self):\n","#        for p in self.densenet.parameters():\n","#            p.requires_grad = False\n","        for c in list(self.densenet.children())[0][:6]:\n","            for p in c.parameters():\n","                p.requires_grad = False\n","\n","class DecoderRNN(nn.Module):\n","    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n","        super(DecoderRNN, self).__init__()\n","\n","        # define the properties\n","        self.embed_size = embed_size\n","        self.hidden_size = hidden_size\n","        self.vocab_size = vocab_size\n","\n","        # embedding layer\n","        self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_size)\n","\n","        # lstm cell\n","        self.lstm_cell = nn.LSTMCell(input_size=embed_size, hidden_size=hidden_size)\n","\n","        # output fully connected layer\n","        self.fc_out = nn.Linear(in_features=self.hidden_size, out_features=self.vocab_size)\n","\n","        # activations\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, features, captions):\n","        # batch size\n","        batch_size = features.size(0)\n","\n","        # init the hidden and cell states to zeros\n","        hidden_state = torch.zeros((batch_size, self.hidden_size)).to(device)\n","        cell_state = torch.zeros((batch_size, self.hidden_size)).to(device)\n","\n","        # define the output tensor placeholder\n","        outputs = torch.empty((batch_size, captions.size(1), self.vocab_size)).to(device)\n","\n","        # embed the captions\n","        captions_embed = self.embed(captions)\n","\n","        # pass the caption word by word\n","        for t in range(captions.size(1)):\n","\n","            # for the first time step the input is the feature vector\n","            if t == 0:\n","                hidden_state, cell_state = self.lstm_cell(features.to(device), (hidden_state, cell_state))\n","\n","            # for the 2nd+ time step, using teacher forcer\n","            else:\n","                hidden_state, cell_state = self.lstm_cell(captions_embed[:, t, :], (hidden_state, cell_state))\n","\n","            # output of the attention mechanism\n","            out = self.fc_out(hidden_state)\n","\n","            # build the output tensor\n","            outputs[:, t, :] = out\n","\n","        return outputs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m5jTg-HKVSKS","colab_type":"code","colab":{}},"source":["import pickle\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset\n","from PIL import Image\n","\n","class ImageDataset(Dataset):\n","    \"\"\"MIT image dataset.\"\"\"\n","\n","    def __init__(self, root_dir, data, transform = None):\n","        self.root_dir  = root_dir\n","        self.dataPtr   = data\n","        self.transform = transform\n","        with open(self.root_dir + \"cleanTrainCap\", \"rb\") as f:\n","            self.trainCap = pickle.load(f)\n","        with open(self.root_dir + \"cleanTrainID\", \"rb\") as f:\n","            self.trainID = pickle.load(f) - 1\n","\n","    def __len__(self):\n","        return len(self.dataPtr)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.root_dir + \"images/\" + self.dataPtr[idx]\n","        image = Image.open(img_name)\n","        if image.mode == 'L':\n","            image = image.convert('RGB')\n","        elif image.mode == 'CMYK':\n","            image = image.convert('RGB')\n","            \n","#        if image.size[0] == 1:\n","#            image = image.repeat(3,1,1)\n","#\n","#        elif  image.size[0] == 4:\n","#            image = image[0].repeat(3,1,1)\n","            \n","        if self.transform:\n","            image = self.transform(image)\n","\n","        imgNum = int(self.dataPtr[idx][3:-4])\n","        captions = self.trainCap[self.trainID == imgNum]\n","        rand    = np.random.randint(captions.shape[0])\n","        caption = captions[rand]\n","\n","        sample = {'image': image, 'caption': torch.from_numpy(np.array(caption))}\n","        return sample"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EpJmA3iYWLPl","colab_type":"code","outputId":"98260c97-034c-4514-b5e5-9d31a513eff0","executionInfo":{"status":"ok","timestamp":1578567253063,"user_tz":-180,"elapsed":1087,"user":{"displayName":"Canberk Baykal","photoUrl":"","userId":"03340276410437477738"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["torch.cuda.is_available()\n"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"ehI6mkzyZ1ZD","colab_type":"code","outputId":"01d85b25-0828-441c-9c08-709890370247","executionInfo":{"status":"error","timestamp":1578567405457,"user_tz":-180,"elapsed":61085,"user":{"displayName":"Canberk Baykal","photoUrl":"","userId":"03340276410437477738"}},"colab":{"base_uri":"https://localhost:8080/","height":197}},"source":["root = \"/content/drive/My Drive/Colab/\"\n","totalList = (os.listdir(root+\"images/\"))\n","#print(len(totalList))"],"execution_count":9,"outputs":[{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-94ebe462722c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/My Drive/Colab/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtotalList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"images/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(len(totalList))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error: '/content/drive/My Drive/Colab/images/'"]}]},{"cell_type":"code","metadata":{"id":"NSWjFELsw7JM","colab_type":"code","outputId":"44909eea-fe88-4662-eb05-e554b832188b","executionInfo":{"status":"error","timestamp":1578509332908,"user_tz":-180,"elapsed":105178,"user":{"displayName":"Abdul Basit Anees","photoUrl":"","userId":"14324490970147572183"}},"colab":{"base_uri":"https://localhost:8080/","height":760}},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","# Load data\n","root = \"/content/drive/My Drive/Colab/\"\n","#totalList = sorted(os.listdir(root+\"images/\"), key=len)\n","totalList = os.listdir(root+\"images/\")\n","lens = [int(len(totalList)*0.5)+1, int(len(totalList)*0.4), int(len(totalList)*0.1)+1]\n","train, test, val = torch.utils.data.dataset.random_split(totalList, lens)\n","\n","dataset_tr   = ImageDataset(root_dir = root, data = train, transform = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.Resize((224,224),interpolation=Image.NEAREST),\n","                                                                                           transforms.ToTensor(),transforms.Normalize(mean = [0.489, 0.456, 0.406], std = [0.229, 0.224,0.225])]))\n","dataset_test = ImageDataset(root_dir = root, data = test, transform = transforms.Compose([transforms.Resize((224,224),interpolation=Image.NEAREST), transforms.ToTensor()]))\n","dataset_val  = ImageDataset(root_dir = root, data = val, transform = transforms.Compose([transforms.Resize((224,224),interpolation=Image.NEAREST),\n","                                                                                         transforms.ToTensor(), transforms.Normalize(mean = [0.489, 0.456, 0.406], std = [0.229, 0.224,0.225])]))\n","dataloader_train = DataLoader(dataset_tr, batch_size=16, shuffle=True)\n","#dataloader_train2 = DataLoader(dataset_tr, batch_size=32, shuffle=True)\n","#dataloader_test  = DataLoader(dataset_test, batch_size=1, shuffle=False)\n","dataloader_val   = DataLoader(dataset_val, batch_size=100, shuffle=True)\n","\n","# Initializations for training\n","losses = list()\n","val_losses = list()\n","decoder = DecoderRNN(embed_size=100, hidden_size=128, vocab_size=1004).to(device)\n","encoder = EncoderCNN(100).to(device)\n","encoder.fine_tune()\n","criterion = nn.CrossEntropyLoss().to(device)\n","\n","vocab_size = 1004\n","encoder_lr = 0.008\n","encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n","                                             lr=encoder_lr)\n","decoder_lr = 0.008\n","decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n","                                             lr=decoder_lr)\n","\n","wordC = pd.read_hdf(root + \"eee443_project_dataset_train.h5\", 'word_code')\n","wordC = wordC.to_dict('split')\n","wordDict = dict(zip(wordC['data'][0], wordC['columns']))\n","# Training starts\n","for epoch in range(1, 5):\n","    print(\"Epoch:\",epoch)\n","    for i, data in enumerate(dataloader_train):\n","        # zero the gradients\n","        decoder.zero_grad()\n","        encoder.zero_grad()\n","\n","        # set decoder and encoder into train mode\n","        encoder.train()\n","        decoder.train()\n","\n","        # Obtain the batch.\n","        images = data[\"image\"]\n","        captions = data[\"caption\"]\n","        # make the captions for targets and teacher forcer\n","        captions_target = captions[:, 1:].long().to(device)\n","        captions_train = captions[:, :captions.shape[1]-1].long().to(device)\n","\n","        # Move batch of images and captions to GPU if CUDA is available.\n","        images = images.to(device)\n","\n","        # Pass the inputs through the CNN-RNN model.\n","        features = encoder(images)\n","        outputs = decoder(features, captions_train)\n","\n","        # Calculate the batch loss\n","        loss = criterion(outputs.view(-1, vocab_size), captions_target.contiguous().view(-1))\n","\n","        # Backward pass\n","        loss.backward()\n","\n","        # Update the parameters in the optimizer\n","        encoder_optimizer.step()\n","        decoder_optimizer.step()\n","\n","       \n","\n","        # - - - Validate - - -\n","        # turn the evaluation mode on\n","        if i % 100 == 0:\n","            with torch.no_grad():\n","\n","                # set the evaluation mode\n","                encoder.eval()\n","                decoder.eval()\n","\n","                # get the validation images and captions\n","                dataVal = next(iter(dataloader_val))\n","                val_images = dataVal[\"image\"]\n","                img = val_images[0]\n","                val_captions = dataVal[\"caption\"]\n","                cap = val_captions[0]\n","                \n","                caption = cap.cpu().detach().numpy()\n","                captionStr = [wordDict[i] for i in caption]\n","                print(captionStr)\n","                # define the captions\n","                captions_target = val_captions[:, 1:].long().to(device)\n","                captions_train = val_captions[:, :-1].long().to(device)\n","\n","                # Move batch of images and captions to GPU if CUDA is available.\n","                val_images = val_images.to(device)\n","\n","                # Pass the inputs through the CNN-RNN model.\n","                val_features = encoder(val_images)\n","                val_outputs = decoder(val_features, captions_train)\n","                \n","                captionOut = val_outputs[0].cpu().detach().numpy().argmax(axis = 1)\n","                captionOutStr = [wordDict[i] for i in captionOut]\n","                print(\"\\n\", captionOutStr)\n","                # Calculate the batch loss.\n","                val_loss = criterion(val_outputs.view(-1, vocab_size), captions_target.contiguous().view(-1))\n","                print(\"Loss of \", i, \"th batch: \", val_loss.item(), sep=\"\")\n","\n","        # append the validation loss and training loss\n","        val_losses.append(val_loss.item())\n","#        losses.append(loss.item())\n","#\n","#        # save the losses\n","#        np.save(\"losses\", np.array(losses))\n","#        np.save(\"val_losses\", np.array(val_losses))\n","#\n","#        # Get training statistics.\n","#        stats = \"Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Val Loss: %.4f\" % (epoch, num_epochs, i_step, total_step, loss.item(), val_loss.item())\n","#\n","#        # Print training statistics (on same line).\n","#        print(\"\\r\" + stats, end=\"\")\n","#        sys.stdout.flush()\n","#\n","#    # Save the weights.\n","#    if epoch % save_every == 0:\n","#        print(\"\\nSaving the model\")\n","#        torch.save(decoder.state_dict(), os.path.join(\"./models\", \"decoder-%d.pth\" % epoch))\n","#        torch.save(encoder.state_dict(), os.path.join(\"./models\", \"encoder-%d.pth\" % epoch))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tables/path.py:118: NaturalNameWarning: object name is a Python keyword: 'from'; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n","  % (name, _warnInfo), NaturalNameWarning)\n","/usr/local/lib/python3.6/dist-packages/tables/path.py:118: NaturalNameWarning: object name is a Python keyword: 'and'; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n","  % (name, _warnInfo), NaturalNameWarning)\n","/usr/local/lib/python3.6/dist-packages/tables/path.py:118: NaturalNameWarning: object name is a Python keyword: 'is'; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n","  % (name, _warnInfo), NaturalNameWarning)\n","/usr/local/lib/python3.6/dist-packages/tables/path.py:118: NaturalNameWarning: object name is a Python keyword: 'in'; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n","  % (name, _warnInfo), NaturalNameWarning)\n","/usr/local/lib/python3.6/dist-packages/tables/path.py:118: NaturalNameWarning: object name is a Python keyword: 'not'; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n","  % (name, _warnInfo), NaturalNameWarning)\n","/usr/local/lib/python3.6/dist-packages/tables/path.py:118: NaturalNameWarning: object name is a Python keyword: 'or'; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n","  % (name, _warnInfo), NaturalNameWarning)\n","/usr/local/lib/python3.6/dist-packages/tables/path.py:118: NaturalNameWarning: object name is a Python keyword: 'with'; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n","  % (name, _warnInfo), NaturalNameWarning)\n","/usr/local/lib/python3.6/dist-packages/tables/path.py:118: NaturalNameWarning: object name is a Python keyword: 'as'; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n","  % (name, _warnInfo), NaturalNameWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1\n","['x_START_', 'a', 'kitchen', 'has', 'a', 'x_UNK_', 'cart', 'in', 'it', 'x_END_', 'x_NULL_', 'x_NULL_', 'x_NULL_', 'x_NULL_', 'x_NULL_', 'x_NULL_', 'x_NULL_']\n","\n"," ['bus', 'couple', 'scooter', 'tower', 'couple', 'into', 'umpire', 'the', 'large', 'x_NULL_', 'x_NULL_', 'x_NULL_', 'x_NULL_', 'x_NULL_', 'x_NULL_', 'x_NULL_']\n","Loss of 0th batch: 6.4116411209106445\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-6761ddfb994e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;31m# zero the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-d9b80cc2f439>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mimg_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"images/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataPtr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2773\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2775\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2777\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}